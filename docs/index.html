<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Article</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.15/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="assets/css/styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">A Multi-class Classification Framework to Confidentially Segregate Habitats</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>




</div>


<div id="section" class="section level1 tabset tabset-fade">
<h1></h1>
<div id="authors" class="section level2">
<h2>Authors</h2>
<div class="container">
<div class="heading-title text-center">
<h3 class="text-uppercase"></h3>
<p class="p-top-30 half-txt"></p>
<br>
<br/>
<br/>
<div class="row">
<div class="col-md-3 col-sm-3">
<div class="team-member">
<div class="team-img">
<img src="assets/authors/luiz.jpg" alt="team member" class="img-responsive" style="margin:auto;top: 0; left:0; right:0; bottom:0;display: block; width:80%"/>
</div>
<div class="team-hover">
<div class="desk">
<h4>Universidade Federal do Rio Grande do Sul</h4>
<p>UFRGS</p>
</div>
<div class="s-link">
<a href="http://www.ufrgs.br/">Departamento de Botânica</a>
</div>
</div>
</div>
<div class="team-title">
<h5>Luíz Fernando Esser</h5>
<p>luizesser@gmail.com</p>
</div>
</div>
<div class="col-md-3 col-sm-3">
<div class="team-member">
<div class="team-img">
<img src="assets/authors/reginaldo.jpg" alt="team member" class="img-responsive" style="margin:auto;top: 0; left:0; right:0; bottom:0;display: block; width:69%"/>
</div>
<div class="team-hover">
<div class="desk">
<h4>Universidade Tecnológica Federal do Paraná</h4>
<p>UTFPR Campo Mourão</p>
</div>
<div class="s-link">
<a href="http://dacom.cm.utfpr.edu.br/portal">Departamento de Computação</a>
</div>
</div>
</div>
<div class="team-title">
<h5>Reginaldo Ré</h5>
<p>reginaldo@utfpr.edu.br</p>
</div>
</div>
<div class="col-md-3 col-sm-3">
<div class="team-member">
<div class="team-img">
<img src="assets/authors/vincent.webp" alt="team member" class="img-responsive" style="margin:auto;top: 0; left:0; right:0; bottom:0;display: block; width:76%"/>
</div>
<div class="team-hover">
<div class="desk">
<h4>Université de Montpellier, Montpellier, France</h4>
<p>Université de Montpellier</p>
</div>
<div class="s-link">
<a href="https://www.umontpellier.fr">UM</a>
</div>
</div>
</div>
<div class="team-title">
<h5>Vincent Montade</h5>
<p>vincent.montade@gmail.com</p>
</div>
</div>
<div class="col-md-3 col-sm-3">
<div class="team-member">
<div class="team-img">
<img src="assets/authors/danilo.png" alt="team member" class="img-responsive" style="margin:auto;top: 0; left:0; right:0; bottom:0;display: block; width:81%"/>
</div>
<div class="team-hover">
<div class="desk">
<h4>Universidade Federal de Minas Gerais, Belo Horizonte, Brazil</h4>
<p>Universidade Federal de Minas Gerais</p>
</div>
<div class="s-link">
<a href="https://ufmg.br/">UFMG</a>
</div>
</div>
</div>
<div class="team-title">
<h5>Danilo Neves</h5>
<p>vincent.montade@gmail.com</p>
</div>
</div>
</div>
</br>
</div>
</div>
<div id="authors-contributions" class="section level3">
<h3>Authors Contributions</h3>
<p>DN and VM conceived the idea and reviewed the final manuscript; LFE and RR designed methodology. LFE analyzed the data and led the writing. All authors gave final approval for publication.</p>
</div>
<div id="acknowledgements" class="section level3">
<h3>Acknowledgements</h3>
<p>This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior - Brasil (CAPES) - Finance Code 001. LFE was supported by CAPES PhD scholarship.</p>
</div>
<div id="data-availability" class="section level3">
<h3>Data Availability</h3>
<p>NTT dataset is fully available at <a href="http://neotroptree.info" class="uri">http://neotroptree.info</a>. R script is available in Supplementary. Model generated for the Atlantic Rainforest is available by request.</p>
</div>
</div>
<div id="abstract" class="section level2 tabset tabset-fade">
<h2>Abstract</h2>
<ol style="list-style-type: decimal">
<li><p>Habitats mapping and detection is a key task in applied (e.g. biodiversity conservation) and theoretical (e.g. scale effects) studies.</p></li>
<li><p>We built a habitats classification framework using machine learning algorithms in R.</p></li>
<li><p>The model built with the framework could confidentially segregate the six habitats from Atlantic 4. Rainforest given the three most diverse tree families (AUC = 0.859).</p></li>
<li><p>Direct implications are the use to map habitats distributions, improving assessments from climate change impacts and RedList of Ecosystems. Indirect applications proposed are the use of the structure of the framework to determine pollen assemblages, bioregionalization assessments and scale effect studies.</p></li>
</ol>
<p><em>Keywords:</em> Atlantic Rainforest, machine learning,</p>
</div>
<div id="introduction" class="section level2 tabset tabset-fade">
<h2>Introduction</h2>
<p>Long before the big-data era, naturalists and explorers traveling around the world, studied and wandered extensively through unexplored regions to understand differences between habitats (hereafter defined as plant assemblages and its associated environmental conditions). To know the flora and the distribution of abundant taxa through space was key to delimitate different physiognomies. Nowadays, community and ecosystem ecologists developed a myriad of different approaches to map habitats and improve knowledge about their mechanisms. One of these approaches is from Olson et al. (2001), where they mapped ecoregions, through overlap of different maps: from bioregions, landforms and vegetation to climate, fire disturbances and vertebrate migrations. The authors point out that ecoregions normally have habitats within, that differ from their assigned biome and that conservation planning for ecoregions must map those less dominant habitats. Their approach, although, is very broad, so they could map bioregions on a global scale. In another study, Tuomisto et al. (2019) crossed environmental, plant survey and remote sense data, which could be more efficient in time as others, but may be not practical depending on your scale resolution. Species distribution modeling (SDMs) is a widely used method to map habitats distribution, through macroecological modeling (MEM; e.g. Carnaval &amp; Moritz, 2008) or stacked species distribution modeling (e.g. Esser, Neves, &amp; Jarenkow, 2019). Nevertheless, approaches using SDMs are somewhat problematic to reproduce when they generate models for every woody species (e.g. Zhang, Slik, &amp; Ma, 2016), since this is very time consuming and may overlook environmental relationships. On the other hand, modeling diagnostic species for each habitat (Esser et al., 2019) would solve that, but then ignores the space-environment continuum which nature is, once assigns different environmental variables for each habitat to be mapped.</p>
<p>Given the problems that arise with different strategies to map and understand habitats, we hereby propose a machine learning approach for habitats detection in R using collectors data that can be applied at any scale, from global to local, allowing habitat mapping to be less time consuming and considering species turnover, which unveils transitional areas (Figure 1). In a former study (Neves et al., 2017), researchers found a possible evidence that the proportion of tree species in key families, could segregate habitats from a tropical forest. We present the final model built for Atlantic Rainforest biodiversity hotspot and a R script with full reproducibility to facilitate framework’s use, expansion, improvement and transferability. Providing these data, we argue that it could improve a variety of ecological assessments, from theoretical studies as scale effect, to applicable studies as conservation planning for climate change scenarios.</p>
<div>
<p><img src="assets/images/framework_overview.png" class="img-responsive" style="margin:auto;top: 0; left:0; right:0; bottom:0;display: block;width:60%"></p>
</div>
<p>Figure 1: Framework overview.</p>
</div>
<div id="methods" class="section level2 tabset tabset-fade">
<h2>Methods</h2>
<div id="data-base" class="section level3">
<h3>Data base</h3>
<p>Input data comprised a subset of NeoTropTree database, which is a database of tree species checklists along the neotropical region. Each site in NeoTropTree is classified according to Oliveira-Filho (2017), presenting a checklist of species within a 5 km radius buffer. Each site is classified in different habitats, and domains, with presence and absence information for each of the species. The subset used in this study comprised only data from Atlantic Rainforest. Habitats from within this domain were reclassified accordingly to Neves et al. (2017). As the database is built with species names, we determined each species family using Taxonstand package in R 3.6.2 (R Development Core Team, 2011; Cayuela, Cerda, Albuquerque, &amp; Golicher, 2012), then summed all species within a family, resulting in a matrix with sites versus families, where each cell is the number of species within a family in a given site. For the purpose of this study we considered Myrtaceae, Fabaceae and Rubiaceae as predictors, which are three botanical families of widely importance in the study region, and with the highest number of species in NeoTropTree.</p>
</div>
<div id="framework-selection" class="section level3">
<h3>Framework selection</h3>
<p>We built two machine learning frameworks, ensembling five classification algorithms with different approaches through an area under the receiver operating characteristic curve (AUC) weighted vote system. The first framework uses a multi-class classification approach, which consists in training the algorithms to segregate between the six habitats from Atlantic Rainforest. The second, is a one-vs-all approach, which trains algorithms to segregate one habitat from the others. Methods for both frameworks were essentially the same to keep comparability. We started by dividing data into train (90%) and test (10%) data, maintaining proportions between habitats. This ensures that we will have a sample of each habitat in both train and test data, as well as retains an independent test data. As habitats have different numbers of records (class imbalance), we sampled records up, that is, we sampled all minor classes, with replacement, until all have the same number of records as the major class. This approach may raise AUC levels, when compared to a scaling down; but, as we do not have many records and this is standardized between frameworks (i.e. the effect will be applied equally in both), we choose to scale them up. We built models from five selected algorithms (Naive Bayes; Random Forest; Boosted Logistic Regression; Neural Network; Support Vector Machines with Radial Basis Function Kernel) with standard parameterization using the train data. Algorithm performance was obtained running 10 times a 10-fold cross-validation, where in each fold we calculated AUC values. Then, we projected the models into the test data and used AUC values to weight each algorithm result. The predicted habitat, given the test data, was that with the greatest number of weighted votes. The only difference between frameworks, relied essentially in the fact that the one-vs-all approach trained algorithms to segregate one habitat from all the others and then projected each of the six models generated (one for each habitat) into the test data, while the multi-class approach trained algorithms to segregate the six habitats from one another. We run frameworks 100 times, selecting a new train and test data at each run. Finally, we compared, using Welch’s t-test, whether the approach that optimized AUC values was the multi-class or the one-vs-all. This approach was proposed so we could minimize algorithms effects. Framework selection was performed using caretEnsemble package (Deane-Mayer &amp; Knowles, 2016) to run models and pROC package (Robin et al., 2011) to calculate multi-class AUC values.</p>
</div>
<div id="algorithms-selection" class="section level3">
<h3>Algorithms Selection</h3>
<p>Afterwards, we selected all classification algorithms, that accepted multi-class, non-binary and non-categorical predictors, available in caret package (Kuhn, 2008). Selected algorithms should also be classified as random forests, logistic regressions, neural networks, support vector machines or naive Bayes approaches. Using the 34 algorithms that we managed to implement (complete list available in Supplementary), we run our multi-class routine 100 times, but instead of making a weighted ensemble, we applied models from each algorithm in the test data and calculated AUC value. This allows us to compare each algorithm contribution to maximize evaluation metrics. The three algorithms with highest mean plus maximum AUC values were selected for the next steps.</p>
</div>
<div id="instance-selection" class="section level3">
<h3>Instance Selection</h3>
<p>We used four instance selection alternatives (a pre-processing method) to see which delivered the best results, when compared to not using (IS0), as in previous steps. Instance selection aims to supply the best training data for the algorithms. Alternatives presented are all based on clusterization and outliers removal. In the first approach (IS1), we generated 50 clusters to each habitat’s train data using k-nearest neighbors and extracted clusters’ centroids, which composed the training data. The second approach to instance selection (IS2) was to generate 50 clusters, as the first, and then exclude 50% of the farthest records from the centroid within its cluster (outliers); remaining records were used as training data. In the third approach (IS3), we calculated only one cluster for the whole habitat and then deleted 50% of the farthest records from the centroid and used the remaining records as training data. The fourth and last approach (IS4), consisted in removing outliers from one cluster, as the third approach, but then calculating 30 clusters with the remaining data and using the centroids from them as training data. The first and fourth approaches solved the imbalance problem we had when considering all data just by calculating the same amount of clusters for each habitat; thus, to sample up was not necessary in those cases. Apart from that, the framework applied in each approach from this section was the same from Algorithms Selection section. We choose not to use an ensemble approach (as in Framework Selection section), once at least one of the algorithms alone could better predict habitats than all together. To compare the four instance selection approaches, we run each of them 100 times, saving AUC values from test predictions and performing an analysis of variance. To evaluate pair means, we used Tukey test.</p>
</div>
<div id="algorithms-tuning" class="section level3">
<h3>Algorithms Tuning</h3>
<p>Algorithms tuning was performed with Conditional Inference Random Forest (caret package code, and hereafter: cforest), Random Forest (caret package code, and hereafter: ranger) and Boosted Logistic Regression (caret package code, and hereafter: LogitBoost). We run the same multi-class framework from algorithm selection, but searching for an optimal range of values that could more frequently comprise the best tuning parameters (optimizing AUC values) for each of the algorithms. The following description comprises the beginning set of values for each parameter from each algorithm. The only parameter for cforest was the number of variables randomly sampled as candidates at each split (mtry), which was set to vary from one to three. An expanded grid (meaning, a data frame with all possible combinations between different vectors) was created for ranger tuning parameters, were the number of variables randomly sampled as candidates at each split (mtry) values ranged from one to three; the split rule varied between minimizing the Gini impurity and randomly selecting a split point (extratrees); and the minimum node size varying from one to 100. The only parameter from LogitBoost is the number of iterations, which was set to vary from one to 200. After running these models and visually analyzing results, we constricted tuning range, and rerun models. This followed until we kept just parameter ranges that consistently delivered good AUC performance, excluding those that only had poor performance. This step was crucial so we could optimize computational time. Final value from cforest’s mtry was one; from ranger, mtry was set to one, minimum node size varying from 25 to 35 and the split rule was not conclusive, thus both approaches were maintained; LogitBoost’s number of iterations ranged from 50 to 100.</p>
</div>
<div id="final-model" class="section level3">
<h3>Final Model</h3>
<p>We ran the final model, built with the result from previous steps, 100 times and kept the best model to make inferences. We controlled randomness in each step of the script by setting a seed equal to one. The final model and script are available in Supplementary.</p>
</div>
</div>
<div id="results" class="section level2 tabset tabset-fade">
<h2>Results</h2>
<p>Multi-class framework had a higher mean AUC (0.633, with standard deviation equals 0.03), then the one-vs-all approach (0.585, with standard deviation equals 0.05). Algorithms with highest mean plus maximum AUC were LogitBoost (1.502), cforest (1.415) and ranger (1.408). Instance selection was better using centroid data (IS1; meanAUC = 0.683, maxAUC = 0.770). Final model built was a multi-class approach using centroid data in a ranger algorithm. As tuning parameters, the number of variables randomly sampled as candidates at each split value was one; the split rule used was the one where the algorithm minimizes the Gini impurity; and the minimum node size was 41. Final AUC reached was 0.859. Detailed results description is presented in Supplementary.</p>
</div>
<div id="discussion" class="section level2 tabset tabset-fade">
<h2>Discussion</h2>
<p>The results presented here demonstrated that it is possible to infer the habitat from Atlantic Rainforest based on the number of tree species in three key botanical families. There are direct applications for the model generated in this study, which will support new studies. This new strategy could improve our predictions over habitats distribution modeling, once one unique approach would build projections for multiple habitats. If we applied a S-SDM method to tree species from Myrtaceae, Fabaceae and Rubiacea, each resulting cell would provide us a list of species, which we could derive to a matrix with sites versus families, where each cell is the number of species within a family in a given site. Applying the model we build in this study, and retaining geographical information, we could map habitats distribution with a high fidelity level. This could allow us to better understand the impacts of climatic change in habitats dynamics, especially when retrieving probabilities for each predicted habitat, highlighting, as well, differences between core and marginal habitats, unlike common bioregions mapping strategies (Edler, Guedes, Zizka, Rosvall, &amp; Antonelli, 2017). This same S-SDM framework could be used to infer habitats size, distribution and changes through time, enabling researchers and practitioners to make fasters systematic assessments of IUCN’s RedList of Ecosystems, improving criterion A and B assessments, i.e. respectively reduction and restriction of geographic distribution (Keith et al., 2013, 2015). Nevertheless, a factor to keep in mind when using this model is that it is scale dependent. Our model was built using NeoTropTree which has a resolution of 5 arc-minutes. If we would consider mapping finer or coarser scales, it would be necessary to build a new model, to not risk increasing error chance.</p>
<p>Indirect effects from this study lies on the possibility to rerun this framework considering different strategies. We previously argued that this approach is scale dependent. In this way, one preposition is to group species records within a finer or coarser grid. This would be possible building a database of presence records (e.g. from GBIF), before generating a raster and retrieving species list within each cell. This approach could also be used to understand scale effect in habitats assembly (Fritsch, Lischke, &amp; Meyer, 2020). Another proposition is to rerun using pollen data (e.g. Montade et al., 2019), this would allow palynologists to infer habitat change in a palynogram, a task which is currently attributed to subjective specialist opinion. With an additional step retrieving variables importance and a stepwise exclusion of unimportant variables, it could be possible to retrieve the most relevant variables (morphotypes) to delimit each habitat. Another extensible possibility is to use the framework to find finer scales from biogeographical regionalization, from kingdoms to realms, to regions and to sub-regions, and their key clades. Regarding those indirect uses to the framework, it is necessary to keep in mind that there is a need, in supervised learning (the set of machine learning strategies we used in this study), that we already have a database properly built, i.e. a database with our variables and the proper class those variables were measured from.</p>
</div>
<div id="references" class="section level2 tabset tabset-fade">
<h2>References</h2>
<p>Carnaval, A. C., &amp; Moritz, C. (2008). Historical climate modelling predicts patterns of current biodiversity in the Brazilian Atlantic forest. Journal of Biogeography, 35, 1187–1201. <a href="doi:10.1111/j.1365-2699.2007.01870.x" class="uri">doi:10.1111/j.1365-2699.2007.01870.x</a></p>
<p>Cayuela, L., Cerda, Í. G. la, Albuquerque, F. S., &amp; Golicher, D. J. (2012). taxonstand: An r package for species names standardisation in vegetation databases. Methods in Ecology and Evolution, 3(6), 1078–1083. <a href="doi:https://doi.org/10.1111/j.2041-210X.2012.00232.x" class="uri">doi:https://doi.org/10.1111/j.2041-210X.2012.00232.x</a></p>
<p>Deane-Mayer, Z. A., &amp; Knowles, J. E. (2016). caretEnsemble: ensembles of caret models. R Package Version, 2(0).</p>
<p>Edler, D., Guedes, T., Zizka, A., Rosvall, M., &amp; Antonelli, A. (2017). Infomap Bioregions: Interactive Mapping of Biogeographical Regions from Species Distributions. Systematic Biology, syw087. <a href="doi:10.1093/sysbio/syw087" class="uri">doi:10.1093/sysbio/syw087</a></p>
<p>Esser, L. F., Neves, D. M., &amp; Jarenkow, J. A. (2019). Habitat-specific impacts of climate change in the Mata Atlântica biodiversity hotspot. Diversity and Distributions, 25(12), 1846–1856. <a href="doi:10.1111/ddi.12984" class="uri">doi:10.1111/ddi.12984</a> Fritsch, M., Lischke, H., &amp; Meyer, K. M. (2020). Scaling methods in ecological modelling. Methods in Ecology and Evolution, 11(11), 1368–1378. <a href="doi:https://doi.org/10.1111/2041-210X.13466" class="uri">doi:https://doi.org/10.1111/2041-210X.13466</a></p>
<p>Keith, D. A., Rodríguez, J. P., Brooks, T. M., Burgman, M. A., Barrow, E. G., Bland, L., … McCarthy, M. A. (2015). The IUCN red list of ecosystems: Motivations, challenges, and applications. Conservation Letters, 8(3), 214–226.</p>
<p>Keith, D. A., Rodríguez, J. P., Rodríguez-Clark, K. M., Nicholson, E., Aapala, K., Alonso, A., … Barrow, E. G. (2013). Scientific foundations for an IUCN Red List of Ecosystems. PLOS One, 8(5), e62111. Kuhn, M. (2008). Building predictive models in R using the caret package. Journal of Statistical Software, 28(5), 1–26.</p>
<p>Montade, V., Ledru, M.-P., Giesecke, T., Flantua, S. G., Behling, H., &amp; Peyron, O. (2019). A new modern pollen dataset describing the Brazilian Atlantic Forest. The Holocene, 29(8), 1253–1262.</p>
<p>Neves, D., Dexter, K., Pennington, R. T., Valente, A. S., Bueno, M., Eisenlohr, P., … Oliveira-Filho, A. de. (2017). Dissecting a biodiversity hotspot: the importance of environmentally marginal habitats in the Atlantic Forest Domain of South America. Diversity and Distributions, 1–12.</p>
<p>Oliveira-Filho, A. T. (2017). NeoTropTree, Flora arbórea da Região Neotropical: Um banco de dados envolvendo biogeografia, diversidade e conservação. Universidade Federal de Minas Gerais. Retrieved from <a href="http://www.neotroptree.info" class="uri">http://www.neotroptree.info</a>.</p>
<p>Olson, D. M., Dinerstein, E., Wikramanayake, E. D., Burgess, N. D., Powell, G. V. N., Underwood, E. C., … Kassem, K. R. (2001). Terrestrial Ecoregions of the World: A New Map of Life on Earth. BioScience, 51(11), 933. <a href="doi:10.1641/0006-3568(2001)051%5B0933:TEOTWA%5D2.0.CO;2" class="uri">doi:10.1641/0006-3568(2001)051[0933:TEOTWA]2.0.CO;2</a></p>
<p>R Development Core Team. (2011). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing (Vol. 1). <a href="doi:10.1007/978-3-540-74686-7" class="uri">doi:10.1007/978-3-540-74686-7</a></p>
<p>Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2011). pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinformatics, 12(1), 1–8.</p>
<p>Tuomisto, H., Cárdenas, G., Ruokolainen, K., Moulatlet, G. M., Figueiredo, F. O. G., Sirén, A., … Zuquim, G. (2019). Discovering floristic and geoecological gradients across Amazonia, (April), 1734–1748. <a href="doi:10.1111/jbi.13627" class="uri">doi:10.1111/jbi.13627</a></p>
<p>Zhang, M.-G., Slik, J. W. F., &amp; Ma, K.-P. (2016). Using species distribution modeling to delineate the botanical richness patterns and phytogeographical regions of China. Scientific Reports, 6, 22400.</p>
</div>
<div id="suplementary" class="section level2">
<h2>Suplementary</h2>
<div id="full-description-of-results" class="section level3">
<h3>Full description of results</h3>
<p>Our first step was to define whether our approach to this problem would be a one-vs-all or a multi-class framework. Welch’s t-test comparing both approaches (Figure S1) unveiled that there is, with more than 99.9 % confidence level, significant difference between the two means, t(197) = 10.795, p &lt; 0.001. The highest mean is from the multi-class framework (0.633, with standard deviation equals 0.03), while the lower mean is from the one-vs-all approach (0.585, with standard deviation equals 0.05). In this way, following procedures were built upon the multi-class framework.</p>
<pre class="r"><code>s1 &lt;- Running_first_step(df, n)</code></pre>
<pre><code>## [1] &quot;Selected Framework: multiclass.&quot;</code></pre>
<p><img src="index_files/figure-html/running_first_step-1.png" width="672" /></p>
<p>Figure 1: Distribution of 100 AUC values from the one-vs-all framework (blue) compared to the distribution of 100 AUC values from the multi-class framework (red).</p>
<p>Boosted Logistic Regression (M) was considered the best algorithm of our Algorithms Selection routine, with a mean AUC of 0.689, reaching maximum value of 0.813 (summing 1.502), followed by Conditional Inference Random Forest (cforest), with a mean AUC of 0.674 and maximum of 0.741 (summing 1.415). Random Forest (ranger) was the third best algorithm with mean AUC of 0.671 and maximum of 0.737 (summing 1.408). Results for all algorithms are summarized in Table 1.</p>
<pre class="r"><code>s2$Result_Second_step  %&gt;%
  datatable()</code></pre>
<div id="htmlwidget-49c54e382314ebabbc84" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-49c54e382314ebabbc84">{"x":{"filter":"none","data":[["5","2","18","6","27","28","29","34","21","19","30","20","22","23","4","26","13","16","1","24","14","11","15","31","10","7","8","17","12","9","33","25","32","3"],["LogitBoost","cforest","ranger","lssvmRadial","svmRadial","svmRadialCost","svmRadialSigma","wsrf","rf","rbfDDA","svmRadialWeights","Rborist","RRF","RRFglobal","LMT","svmPoly","naive_bayes","pcaNNet","avNNet","svmLinear","nb","monmlp","nnet","vglmAdjCat","mlpWeightDecayML","mlp","mlpML","polr","multinom","mlpWeightDecay","vglmCumulative","svmLinear2","vglmContRatio","dnn"],[0.688792087075853,0.674226675733423,0.671433508892517,0.6687682957381,0.664226088963268,0.664010470592286,0.662946212488217,0.657244667513224,0.66263068415565,0.649082352470342,0.650233298371794,0.660084380126047,0.653030452453529,0.653256698398567,0.638567312978919,0.637227875488334,0.634621165197076,0.624007287746492,0.621495588301088,0.619046955549823,0.633567667968309,0.622569173822379,0.613057062720356,0.600425616506251,0.609461740270276,0.602935698772912,0.604105461515043,0.611963756260484,0.600726519581783,0.606090385497943,0.604143666496014,0.590735063269814,0.58326955648565,0.506936472059954],[0.81343537414966,0.741109549594705,0.737017900378224,0.734579837616275,0.732587305107548,0.726920905480285,0.726596482315106,0.723226940500895,0.717454170760514,0.730846699284621,0.71955971441464,0.70843095650788,0.715112346168352,0.705737139728368,0.71922984735063,0.719163821508626,0.710552552236088,0.710167057788515,0.6995879676379,0.695924114496989,0.677799090248483,0.687226018096463,0.694538782055651,0.695530864896587,0.676225716104259,0.679510899332087,0.673547226128198,0.663900707304891,0.668422241026829,0.663058056428502,0.663450545805472,0.649323741935078,0.648013820162943,0.619829877724615],[1.50222746122551,1.41533622532813,1.40845140927074,1.40334813335437,1.39681339407082,1.39093137607257,1.38954269480332,1.38047160801412,1.38008485491616,1.37992905175496,1.36979301278643,1.36851533663393,1.36814279862188,1.35899383812694,1.35779716032955,1.35639169699696,1.34517371743316,1.33417434553501,1.32108355593899,1.31497107004681,1.31136675821679,1.30979519191884,1.30759584477601,1.29595648140284,1.28568745637453,1.282446598105,1.27765268764324,1.27586446356537,1.26914876060861,1.26914844192644,1.26759421230149,1.24005880520489,1.23128337664859,1.12676634978457]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Algorithm<\/th>\n      <th>Mean<\/th>\n      <th>Max<\/th>\n      <th>Sum<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Table 1: AUC values for the algorithms selection. Algorithms names are encoded as caret package values for the ‘method’ argument from ‘train’ function.</p>
<p>The four instance selection builds returned significantly different results. Analysis of variance showed that there was a significant effect from different instance selection approaches in AUC value [F(4) = 31.09, p &lt; 0.01]. Post hoc Tukey test indicated that there was a significant difference between pairs of groups (p &lt; 0.01), except between IS0 (mean = 0.687, max = 0.747), IS1 (mean = 0.685, max = 0.770) and IS2 (mean = 0.687, max = 0.750) mean values (p &gt; 0.95). Mean and maximum AUC values for IS3 were 0.672 and 0.734, while for IS4 those values were 0.653 and 0.741, respectively. As IS0, IS1 and IS2 presented the same means, we decided to adopt the the highest sum between mean and maximum AUC values and use IS1 in the final model.</p>
<pre class="r"><code>s3 &lt;- Running_third_step(df, s1, s2, nclust, n_clust/2, n)</code></pre>
<pre><code>## [1] &quot;Instance Selection Selected: IS1.&quot;</code></pre>
<p><img src="index_files/figure-html/running_third_step-1.png" width="672" /></p>
<p>Figure 2: Tukey post hoc test results comparing means between pairs of instance selection approaches. IS1 returned the greatest sum between mean and maximum AUC, thus was used in the next steps.</p>
<p>Concerning cforest’s tuning, as we had a low number of variables, the randomly sampled number of variables as candidates at each split could only be one, once when we considered more than one, AUC values dropped. Ranger has also this same parameter, since both are Random Forest approaches, but has also two more different parameters: split rule and the minimum node size. Split rules had different effects in the model: when using a random approach (extratrees) and one variable at each split, values of AUC would vary only between 0.82 and 0.83 independent of the minimum node size; on the other hand, when considering two or three variables, AUC values would increase with minimum node size until reach a plateau starting at 40 as minimum node size. In the opposite way, when considering a splitting approach that minimizes Gini impurity, AUC values would increase with minimum node size until reach a plateau starting at 40 as minimum node size, but as the number of variables at each split increased, maximum AUC decreased. In this way, we decided to set ranger parameters as only one variable per split, minimizing Gini impurity and with a minimum node size of 40. The relationship between AUC value and the number of iterations in LogitBoost presented a clear logarithmic shape, which led us to tune it with a minimum of 50 iterations.</p>
<pre class="r"><code>s4 &lt;- Running_fourth_step(df)</code></pre>
<p><img src="index_files/figure-html/running_fourth_step-1.png" width="672" style="display: block; margin: auto;" /><img src="index_files/figure-html/running_fourth_step-2.png" width="672" style="display: block; margin: auto;" /><img src="index_files/figure-html/running_fourth_step-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>Figure 3: Algorithms tuning results.</p>
<p>From all 100 runs, the maximum AUC value was 0.859, with mean AUC equals 0.816, standard deviation of 0.019 and minimum value reaching 0.761. Final model selected was built using ranger algorithm, with AUC equals 0.859. Number of variables randomly sampled as candidates at each split was one; the split rule used was the one where we minimize the Gini impurity; and the minimum node size was 41.</p>
<pre><code>## Random Forest 
## 
## 600 samples
##   3 predictor
##   6 classes: &#39;High.Elevation&#39;, &#39;Rainforest&#39;, &#39;Restinga&#39;, &#39;Riverine&#39;, &#39;Rocky&#39;, &#39;Semideciduous&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 540, 540, 540, 540, 540, 540, ... 
## Addtional sampling using up-sampling
## 
## Resampling results across tuning parameters:
## 
##   min.node.size  logLoss  AUC    prAUC  Accuracy  Kappa  Mean_F1
##    40            1.47     0.769  0.391  0.412     0.294  0.399  
##    41            1.47     0.769  0.388  0.415     0.298  0.413  
##    42            1.47     0.770  0.391  0.405     0.286  0.405  
##    43            1.47     0.768  0.390  0.402     0.282  0.403  
##    44            1.47     0.768  0.386  0.418     0.302  0.418  
##    45            1.47     0.768  0.390  0.410     0.292  0.410  
##    46            1.47     0.768  0.390  0.412     0.294  0.409  
##    47            1.48     0.767  0.389  0.402     0.282  0.403  
##    48            1.47     0.769  0.392  0.407     0.288  0.405  
##    49            1.48     0.767  0.386  0.407     0.288  0.406  
##    50            1.48     0.767  0.385  0.405     0.286  0.404  
##    51            1.48     0.767  0.386  0.408     0.290  0.403  
##    52            1.48     0.767  0.389  0.403     0.284  0.401  
##    53            1.48     0.764  0.386  0.400     0.280  0.397  
##    54            1.48     0.767  0.386  0.405     0.286  0.404  
##    55            1.48     0.767  0.389  0.407     0.288  0.403  
##    56            1.49     0.765  0.385  0.395     0.274  0.393  
##    57            1.49     0.763  0.384  0.403     0.284  0.402  
##    58            1.48     0.767  0.387  0.405     0.286  0.404  
##    59            1.49     0.766  0.387  0.407     0.288  0.404  
##    60            1.48     0.767  0.388  0.400     0.280  0.389  
##    61            1.49     0.764  0.384  0.405     0.286  0.401  
##    62            1.49     0.765  0.387  0.407     0.288  0.406  
##    63            1.49     0.765  0.383  0.400     0.280  0.395  
##    64            1.49     0.763  0.382  0.405     0.286  0.394  
##    65            1.49     0.765  0.383  0.410     0.292  0.401  
##    66            1.49     0.765  0.388  0.400     0.280  0.389  
##    67            1.50     0.763  0.386  0.408     0.290  0.406  
##    68            1.50     0.763  0.381  0.398     0.278  0.395  
##    69            1.50     0.763  0.385  0.407     0.288  0.402  
##    70            1.50     0.763  0.387  0.405     0.286  0.409  
##    71            1.50     0.763  0.385  0.405     0.286  0.394  
##    72            1.50     0.763  0.382  0.403     0.284  0.391  
##    73            1.50     0.762  0.382  0.407     0.288  0.404  
##    74            1.50     0.762  0.382  0.405     0.286  0.393  
##    75            1.50     0.760  0.379  0.400     0.280  0.391  
##    76            1.50     0.761  0.382  0.402     0.282  0.394  
##    77            1.51     0.761  0.381  0.402     0.282  0.389  
##    78            1.51     0.762  0.384  0.395     0.274  0.385  
##    79            1.51     0.760  0.380  0.402     0.282  0.391  
##    80            1.51     0.760  0.378  0.395     0.274  0.382  
##    81            1.51     0.760  0.380  0.390     0.268  0.377  
##    82            1.51     0.758  0.377  0.392     0.270  0.380  
##    83            1.51     0.762  0.381  0.405     0.286  0.401  
##    84            1.51     0.759  0.376  0.397     0.276  0.385  
##    85            1.51     0.762  0.381  0.398     0.278  0.385  
##    86            1.51     0.759  0.381  0.390     0.268  0.377  
##    87            1.52     0.759  0.376  0.400     0.280  0.388  
##    88            1.52     0.756  0.376  0.393     0.272  0.381  
##    89            1.52     0.757  0.379  0.410     0.292  0.398  
##    90            1.52     0.759  0.379  0.390     0.268  0.377  
##    91            1.52     0.758  0.378  0.393     0.272  0.381  
##    92            1.52     0.758  0.377  0.397     0.276  0.386  
##    93            1.52     0.756  0.376  0.390     0.268  0.379  
##    94            1.52     0.757  0.375  0.397     0.276  0.386  
##    95            1.52     0.756  0.370  0.395     0.274  0.383  
##    96            1.52     0.757  0.377  0.392     0.270  0.379  
##    97            1.52     0.757  0.375  0.388     0.266  0.376  
##    98            1.52     0.756  0.371  0.398     0.278  0.386  
##    99            1.53     0.755  0.376  0.390     0.268  0.377  
##   100            1.53     0.757  0.375  0.387     0.264  0.373  
##   Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value
##   0.412             0.882             0.412                0.884              
##   0.415             0.883             0.419                0.884              
##   0.405             0.881             0.412                0.882              
##   0.402             0.880             0.410                0.882              
##   0.418             0.884             0.421                0.885              
##   0.410             0.882             0.422                0.883              
##   0.412             0.882             0.420                0.884              
##   0.402             0.880             0.410                0.882              
##   0.407             0.881             0.416                0.883              
##   0.407             0.881             0.413                0.883              
##   0.405             0.881             0.421                0.882              
##   0.408             0.882             0.418                0.883              
##   0.403             0.881             0.408                0.882              
##   0.400             0.880             0.408                0.881              
##   0.405             0.881             0.413                0.882              
##   0.407             0.881             0.417                0.883              
##   0.395             0.879             0.401                0.880              
##   0.403             0.881             0.415                0.882              
##   0.405             0.881             0.412                0.882              
##   0.407             0.881             0.424                0.882              
##   0.400             0.880             0.413                0.881              
##   0.405             0.881             0.415                0.882              
##   0.407             0.881             0.429                0.883              
##   0.400             0.880             0.411                0.881              
##   0.405             0.881             0.418                0.882              
##   0.410             0.882             0.439                0.883              
##   0.400             0.880             0.412                0.881              
##   0.408             0.882             0.423                0.883              
##   0.398             0.880             0.417                0.881              
##   0.407             0.881             0.433                0.883              
##   0.405             0.881             0.427                0.882              
##   0.405             0.881             0.423                0.882              
##   0.403             0.881             0.424                0.882              
##   0.407             0.881             0.423                0.883              
##   0.405             0.881             0.426                0.883              
##   0.400             0.880             0.428                0.881              
##   0.402             0.880             0.423                0.882              
##   0.402             0.880             0.429                0.882              
##   0.395             0.879             0.429                0.880              
##   0.402             0.880             0.430                0.882              
##   0.395             0.879             0.427                0.881              
##   0.390             0.878             0.419                0.880              
##   0.392             0.878             0.420                0.880              
##   0.405             0.881             0.434                0.882              
##   0.397             0.879             0.425                0.881              
##   0.398             0.880             0.433                0.881              
##   0.390             0.878             0.414                0.880              
##   0.400             0.880             0.436                0.881              
##   0.393             0.879             0.427                0.880              
##   0.410             0.882             0.446                0.884              
##   0.390             0.878             0.414                0.880              
##   0.393             0.879             0.428                0.880              
##   0.397             0.879             0.432                0.881              
##   0.390             0.878             0.424                0.879              
##   0.397             0.879             0.426                0.881              
##   0.395             0.879             0.440                0.880              
##   0.392             0.878             0.419                0.880              
##   0.388             0.878             0.433                0.879              
##   0.398             0.880             0.440                0.881              
##   0.390             0.878             0.433                0.879              
##   0.387             0.877             0.424                0.879              
##   Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy
##   0.412           0.412        0.0686               0.647                 
##   0.419           0.415        0.0692               0.649                 
##   0.412           0.405        0.0675               0.643                 
##   0.410           0.402        0.0669               0.641                 
##   0.421           0.418        0.0697               0.651                 
##   0.422           0.410        0.0683               0.646                 
##   0.420           0.412        0.0686               0.647                 
##   0.410           0.402        0.0669               0.641                 
##   0.416           0.407        0.0678               0.644                 
##   0.413           0.407        0.0678               0.644                 
##   0.421           0.405        0.0675               0.643                 
##   0.418           0.408        0.0681               0.645                 
##   0.408           0.403        0.0672               0.642                 
##   0.408           0.400        0.0667               0.640                 
##   0.413           0.405        0.0675               0.643                 
##   0.417           0.407        0.0678               0.644                 
##   0.401           0.395        0.0658               0.637                 
##   0.415           0.403        0.0672               0.642                 
##   0.412           0.405        0.0675               0.643                 
##   0.424           0.407        0.0678               0.644                 
##   0.413           0.400        0.0667               0.640                 
##   0.415           0.405        0.0675               0.643                 
##   0.429           0.407        0.0678               0.644                 
##   0.411           0.400        0.0667               0.640                 
##   0.418           0.405        0.0675               0.643                 
##   0.439           0.410        0.0683               0.646                 
##   0.412           0.400        0.0667               0.640                 
##   0.423           0.408        0.0681               0.645                 
##   0.417           0.398        0.0664               0.639                 
##   0.433           0.407        0.0678               0.644                 
##   0.427           0.405        0.0675               0.643                 
##   0.423           0.405        0.0675               0.643                 
##   0.424           0.403        0.0672               0.642                 
##   0.423           0.407        0.0678               0.644                 
##   0.426           0.405        0.0675               0.643                 
##   0.428           0.400        0.0667               0.640                 
##   0.423           0.402        0.0669               0.641                 
##   0.429           0.402        0.0669               0.641                 
##   0.429           0.395        0.0658               0.637                 
##   0.430           0.402        0.0669               0.641                 
##   0.427           0.395        0.0658               0.637                 
##   0.419           0.390        0.0650               0.634                 
##   0.420           0.392        0.0653               0.635                 
##   0.434           0.405        0.0675               0.643                 
##   0.425           0.397        0.0661               0.638                 
##   0.433           0.398        0.0664               0.639                 
##   0.414           0.390        0.0650               0.634                 
##   0.436           0.400        0.0667               0.640                 
##   0.427           0.393        0.0656               0.636                 
##   0.446           0.410        0.0683               0.646                 
##   0.414           0.390        0.0650               0.634                 
##   0.428           0.393        0.0656               0.636                 
##   0.432           0.397        0.0661               0.638                 
##   0.424           0.390        0.0650               0.634                 
##   0.426           0.397        0.0661               0.638                 
##   0.440           0.395        0.0658               0.637                 
##   0.419           0.392        0.0653               0.635                 
##   0.433           0.388        0.0647               0.633                 
##   0.440           0.398        0.0664               0.639                 
##   0.433           0.390        0.0650               0.634                 
##   0.424           0.387        0.0644               0.632                 
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 1
## Tuning
##  parameter &#39;splitrule&#39; was held constant at a value of gini
## AUC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 1, splitrule = gini
##  and min.node.size = 42.</code></pre>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
