<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Supplementary</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/spacelab.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.2/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.15/datatables.js"></script>
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="assets/css/styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 52px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h2 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h3 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h4 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h5 {
  padding-top: 57px;
  margin-top: -57px;
}
.section h6 {
  padding-top: 57px;
  margin-top: -57px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">A Multi-class Classification Framework to Confidentially Segregate Habitats</a>
</li>
<li>
  <a href="suplementary.html">Suplementary</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>




</div>


<div id="full-description-of-results" class="section level2">
<h2>Full description of results</h2>
<p>Our first step was to define whether our approach to this problem would be a one-vs-all or a multi-class framework. Welch’s t-test comparing both approaches (Figure S1) unveiled that there is, with more than 99.9 % confidence level, significant difference between the two means, t(197) = 10.795, p &lt; 0.001. The highest mean is from the multi-class framework (0.633, with standard deviation equals 0.03), while the lower mean is from the one-vs-all approach (0.585, with standard deviation equals 0.05). In this way, following procedures were built upon the multi-class framework.</p>
<pre class="r"><code>s1 &lt;- Running_first_step(df, n)</code></pre>
<pre><code>## [1] &quot;Selected Framework: multiclass.&quot;</code></pre>
<p><img src="suplementary_files/figure-html/running_first_step-1.png" width="672" /> Figure 1: Distribution of 100 AUC values from the one-vs-all framework (blue) compared to the distribution of 100 AUC values from the multi-class framework (red).</p>
<p>Boosted Logistic Regression (M) was considered the best algorithm of our Algorithms Selection routine, with a mean AUC of 0.7284, reaching maximum value of 0.7402 (summing 1.4686), followed by Conditional Inference Random Forest (cforest), with a mean AUC of 0.674 and maximum of 0.741 (summing 1.415). Random Forest (ranger) was the third best algorithm with mean AUC of 0.671 and maximum of 0.737 (summing 1.408). Results for all algorithms are summarized in Table 1.</p>
<pre class="r"><code>s2$Result_Second_step  %&gt;%
  datatable()</code></pre>
<div id="htmlwidget-3caf13adf109faf7666a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3caf13adf109faf7666a">{"x":{"filter":"none","data":[["5","2","20","34","18","16","22","30","19","6","21","23","27","28","1","29","13","24","9","14","26","7","4","12","31","11","15","25","17","10","8","33","3","32"],["LogitBoost","cforest","Rborist","wsrf","ranger","pcaNNet","RRF","svmRadialWeights","rbfDDA","lssvmRadial","rf","RRFglobal","svmRadial","svmRadialCost","avNNet","svmRadialSigma","naive_bayes","svmLinear","mlpWeightDecay","nb","svmPoly","mlp","LMT","multinom","vglmAdjCat","monmlp","nnet","svmLinear2","polr","mlpWeightDecayML","mlpML","vglmCumulative","dnn","vglmContRatio"],[0.725565529661379,0.7284034220016,0.707910111705658,0.701035376254675,0.694233447489183,0.667946524660425,0.689733733053571,0.680327241671709,0.670848941943746,0.67668393203987,0.678714753158073,0.681964218991884,0.667764051375725,0.670578402891156,0.658010142254407,0.663092956800784,0.654073056510641,0.654426322290695,0.637208906515587,0.643250336595478,0.639163338286145,0.644266231032425,0.642273303120132,0.635936351472789,0.625446065467995,0.629517577501721,0.628011432886602,0.621428972761631,0.621925829019284,0.604001416374898,0.566528022010478,0.597748692240933,0.550815169236222,0.574090289989413],[0.746148312600882,0.740186175223287,0.713816173954501,0.711920888877704,0.717007972034963,0.730032390761136,0.703017339910053,0.705035287474559,0.713722120334806,0.70144235793696,0.693492718981923,0.68675246006555,0.678146735181148,0.673286119962908,0.685820330017361,0.67828146075447,0.680451295717827,0.669044130009042,0.678304464066947,0.666476540902992,0.661195587663199,0.652958793967566,0.654932395165189,0.660438890094761,0.663231505336768,0.637520560985473,0.635684289344883,0.634901630583142,0.626836232318688,0.614880246459194,0.633056044020956,0.601776052332733,0.601630338472444,0.577745402745403],[1.47171384226226,1.46858959722489,1.42172628566016,1.41295626513238,1.41124141952415,1.39797891542156,1.39275107296362,1.38536252914627,1.38457106227855,1.37812628997683,1.37220747214,1.36871667905743,1.34591078655687,1.34386452285406,1.34383047227177,1.34137441755525,1.33452435222847,1.32347045229974,1.31551337058253,1.30972687749847,1.30035892594934,1.29722502499999,1.29720569828532,1.29637524156755,1.28867757080476,1.26703813848719,1.26369572223148,1.25633060334477,1.24876206133797,1.21888166283409,1.19958406603143,1.19952474457367,1.15244550770867,1.15183569273482]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Algorithm<\/th>\n      <th>Mean<\/th>\n      <th>Max<\/th>\n      <th>Sum<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"columnDefs":[{"className":"dt-right","targets":[2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>
<p>Table 1: AUC values for the algorithms selection. Algorithms names are encoded as caret package values for the ‘method’ argument from ‘train’ function.</p>
<p>The four instance selection builds returned significantly different results. Analysis of variance showed that there was a significant effect from different instance selection approaches in AUC value [F(4) = 31.09, p &lt; 0.01]. Post hoc Tukey test indicated that there was a significant difference between pairs of groups (p &lt; 0.01), except between IS0 (mean = 0.687, max = 0.747), IS1 (mean = 0.685, max = 0.770) and IS2 (mean = 0.687, max = 0.750) mean values (p &gt; 0.95). Mean and maximum AUC values for IS3 were 0.672 and 0.734, while for IS4 those values were 0.653 and 0.741, respectively. As IS0, IS1 and IS2 presented the same means, we decided to adopt the the highest sum between mean and maximum AUC values and use IS1 in the final model.</p>
<pre class="r"><code>s3 &lt;- Running_third_step(df, s1, s2, nclust, n_clust/2, n)</code></pre>
<pre><code>## [1] &quot;Instance Selection Selected: IS0.&quot;</code></pre>
<p><img src="suplementary_files/figure-html/running_third_step-1.png" width="672" /></p>
<p>Figure 2: Tukey post hoc test results comparing means between pairs of instance selection approaches. IS1 returned the greatest sum between mean and maximum AUC, thus was used in the next steps.</p>
<p>Concerning cforest’s tuning, as we had a low number of variables, the randomly sampled number of variables as candidates at each split could only be one, once when we considered more than one, AUC values dropped. Ranger has also this same parameter, since both are Random Forest approaches, but has also two more different parameters: split rule and the minimum node size. Split rules had different effects in the model: when using a random approach (extratrees) and one variable at each split, values of AUC would vary only between 0.82 and 0.83 independent of the minimum node size; on the other hand, when considering two or three variables, AUC values would increase with minimum node size until reach a plateau starting at 40 as minimum node size. In the opposite way, when considering a splitting approach that minimizes Gini impurity, AUC values would increase with minimum node size until reach a plateau starting at 40 as minimum node size, but as the number of variables at each split increased, maximum AUC decreased. In this way, we decided to set ranger parameters as only one variable per split, minimizing Gini impurity and with a minimum node size of 40. The relationship between AUC value and the number of iterations in LogitBoost presented a clear logarithmic shape, which led us to tune it with a minimum of 50 iterations.</p>
<pre class="r"><code>s4 &lt;- Running_fourth_step(df)</code></pre>
<pre><code>## Warning in Running_fourth_step(df): Loading /home/reginaldo/UTFPR/projetos/
## Habitat_Classification/data/computed/Fourth_step_result_GCF.rds.</code></pre>
<p><img src="suplementary_files/figure-html/running_fourth_step-1.png" width="672" style="display: block; margin: auto;" /><img src="suplementary_files/figure-html/running_fourth_step-2.png" width="672" style="display: block; margin: auto;" /><img src="suplementary_files/figure-html/running_fourth_step-3.png" width="672" style="display: block; margin: auto;" /></p>
<p>Figure 3: Algorithms tuning results.</p>
<pre><code>From all 100 runs, the maximum AUC value was 0.859, with mean AUC equals 0.816, standard deviation of 0.019 and minimum value reaching 0.761. Final model selected was built using ranger algorithm, with AUC equals 0.859. Number of variables randomly sampled as candidates at each split was one; the split rule used was the one where we minimize the Gini impurity; and the minimum node size was 41. </code></pre>
<pre class="r"><code>s5 &lt;- Running_fifth_step(df, s3, n)</code></pre>
<pre><code>## Random Forest 
## 
## 1580 samples
##    3 predictor
##    6 classes: &#39;High.Elevation&#39;, &#39;Rainforest&#39;, &#39;Restinga&#39;, &#39;Riverine&#39;, &#39;Rocky&#39;, &#39;Semideciduous&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 1422, 1422, 1423, 1421, 1422, 1424, ... 
## Addtional sampling using up-sampling
## 
## Resampling results across tuning parameters:
## 
##   min.node.size  logLoss  AUC     prAUC   Accuracy  Kappa   Mean_F1
##    40            1.254    0.8305  0.4508  0.5095    0.3920  0.4687 
##    41            1.255    0.8306  0.4498  0.5031    0.3845  0.4635 
##    42            1.253    0.8317  0.4516  0.5121    0.3964  0.4743 
##    43            1.255    0.8311  0.4460  0.4962    0.3759  0.4567 
##    44            1.249    0.8345  0.4548  0.5095    0.3911  0.4704 
##    45            1.251    0.8342  0.4535  0.5164    0.4021  0.4783 
##    46            1.258    0.8316  0.4463  0.5012    0.3844  0.4615 
##    47            1.259    0.8309  0.4486  0.5095    0.3921  0.4695 
##    48            1.257    0.8322  0.4537  0.5070    0.3900  0.4670 
##    49            1.262    0.8302  0.4459  0.4942    0.3751  0.4560 
##    50            1.255    0.8339  0.4488  0.5082    0.3935  0.4741 
##    51            1.253    0.8362  0.4538  0.5171    0.4030  0.4797 
##    52            1.259    0.8318  0.4488  0.5094    0.3931  0.4724 
##    53            1.263    0.8306  0.4487  0.5019    0.3852  0.4673 
##    54            1.253    0.8349  0.4542  0.5171    0.4048  0.4813 
##    55            1.262    0.8320  0.4495  0.5037    0.3870  0.4675 
##    56            1.258    0.8340  0.4528  0.5164    0.4023  0.4794 
##    57            1.262    0.8326  0.4529  0.5095    0.3951  0.4731 
##    58            1.263    0.8330  0.4531  0.5133    0.3999  0.4771 
##    59            1.267    0.8322  0.4470  0.5012    0.3863  0.4691 
##    60            1.270    0.8318  0.4475  0.4974    0.3820  0.4624 
##    61            1.269    0.8323  0.4521  0.5018    0.3868  0.4683 
##    62            1.267    0.8328  0.4473  0.5036    0.3884  0.4692 
##    63            1.269    0.8332  0.4521  0.5132    0.4011  0.4808 
##    64            1.269    0.8327  0.4525  0.5101    0.3977  0.4774 
##    65            1.263    0.8360  0.4551  0.5051    0.3915  0.4714 
##    66            1.275    0.8305  0.4464  0.4980    0.3824  0.4604 
##    67            1.268    0.8339  0.4508  0.5158    0.4025  0.4818 
##    68            1.273    0.8312  0.4463  0.5025    0.3884  0.4687 
##    69            1.276    0.8322  0.4475  0.5114    0.3979  0.4769 
##    70            1.272    0.8325  0.4533  0.4918    0.3756  0.4568 
##    71            1.277    0.8321  0.4495  0.4974    0.3839  0.4636 
##    72            1.271    0.8362  0.4560  0.5094    0.3960  0.4773 
##    73            1.274    0.8327  0.4503  0.4955    0.3794  0.4644 
##    74            1.277    0.8327  0.4470  0.4974    0.3825  0.4640 
##    75            1.279    0.8330  0.4542  0.4980    0.3858  0.4692 
##    76            1.279    0.8314  0.4501  0.4955    0.3807  0.4605 
##    77            1.276    0.8329  0.4473  0.4987    0.3834  0.4651 
##    78            1.281    0.8318  0.4484  0.5012    0.3867  0.4659 
##    79            1.284    0.8310  0.4478  0.4905    0.3768  0.4573 
##    80            1.282    0.8324  0.4482  0.5012    0.3872  0.4695 
##    81            1.284    0.8324  0.4438  0.5007    0.3867  0.4715 
##    82            1.280    0.8327  0.4514  0.5018    0.3886  0.4666 
##    83            1.280    0.8337  0.4490  0.5012    0.3884  0.4676 
##    84            1.286    0.8309  0.4427  0.4993    0.3856  0.4668 
##    85            1.285    0.8322  0.4491  0.4981    0.3830  0.4623 
##    86            1.288    0.8324  0.4503  0.4968    0.3834  0.4671 
##    87            1.282    0.8347  0.4492  0.4956    0.3808  0.4644 
##    88            1.289    0.8313  0.4455  0.4924    0.3778  0.4607 
##    89            1.282    0.8342  0.4523  0.5094    0.3978  0.4778 
##    90            1.291    0.8310  0.4458  0.5025    0.3910  0.4716 
##    91            1.285    0.8331  0.4482  0.4936    0.3795  0.4600 
##    92            1.290    0.8305  0.4447  0.4937    0.3794  0.4602 
##    93            1.288    0.8328  0.4482  0.4955    0.3797  0.4608 
##    94            1.297    0.8285  0.4409  0.4835    0.3677  0.4530 
##    95            1.291    0.8327  0.4493  0.4924    0.3792  0.4602 
##    96            1.290    0.8327  0.4456  0.4981    0.3844  0.4695 
##    97            1.293    0.8319  0.4457  0.4975    0.3849  0.4679 
##    98            1.293    0.8315  0.4440  0.4917    0.3771  0.4604 
##    99            1.297    0.8296  0.4450  0.4868    0.3716  0.4547 
##   100            1.293    0.8330  0.4461  0.4841    0.3690  0.4520 
##   Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value
##   0.5047            0.9019            0.4686               0.8986             
##   0.5000            0.9005            0.4644               0.8973             
##   0.5157            0.9023            0.4722               0.8994             
##   0.4932            0.8989            0.4574               0.8960             
##   0.5080            0.9014            0.4703               0.8984             
##   0.5210            0.9034            0.4783               0.9005             
##   0.5022            0.9006            0.4616               0.8975             
##   0.5073            0.9017            0.4691               0.8987             
##   0.5065            0.9015            0.4674               0.8984             
##   0.4958            0.8987            0.4558               0.8959             
##   0.5193            0.9022            0.4755               0.8988             
##   0.5234            0.9036            0.4786               0.9004             
##   0.5137            0.9018            0.4698               0.8987             
##   0.5083            0.9005            0.4673               0.8974             
##   0.5269            0.9040            0.4819               0.9008             
##   0.5066            0.9009            0.4696               0.8978             
##   0.5246            0.9036            0.4787               0.9004             
##   0.5148            0.9024            0.4750               0.8991             
##   0.5217            0.9032            0.4776               0.9000             
##   0.5154            0.9007            0.4691               0.8977             
##   0.5060            0.9004            0.4660               0.8970             
##   0.5127            0.9008            0.4709               0.8978             
##   0.5134            0.9012            0.4684               0.8980             
##   0.5287            0.9034            0.4804               0.9001             
##   0.5248            0.9028            0.4792               0.8995             
##   0.5182            0.9018            0.4738               0.8985             
##   0.5044            0.9003            0.4624               0.8973             
##   0.5281            0.9036            0.4813               0.9003             
##   0.5144            0.9013            0.4712               0.8980             
##   0.5206            0.9028            0.4779               0.8996             
##   0.5018            0.8993            0.4601               0.8960             
##   0.5121            0.9008            0.4692               0.8976             
##   0.5246            0.9023            0.4785               0.8993             
##   0.5108            0.8995            0.4645               0.8965             
##   0.5103            0.9002            0.4678               0.8972             
##   0.5202            0.9008            0.4735               0.8977             
##   0.5067            0.9002            0.4666               0.8970             
##   0.5107            0.9004            0.4660               0.8972             
##   0.5106            0.9011            0.4685               0.8979             
##   0.5054            0.8995            0.4641               0.8965             
##   0.5184            0.9009            0.4732               0.8979             
##   0.5221            0.9007            0.4738               0.8978             
##   0.5144            0.9016            0.4708               0.8982             
##   0.5167            0.9013            0.4702               0.8982             
##   0.5148            0.9009            0.4733               0.8978             
##   0.5095            0.9005            0.4647               0.8972             
##   0.5179            0.9003            0.4704               0.8974             
##   0.5115            0.8999            0.4675               0.8969             
##   0.5093            0.8996            0.4654               0.8963             
##   0.5268            0.9029            0.4811               0.8996             
##   0.5214            0.9019            0.4779               0.8986             
##   0.5066            0.8998            0.4643               0.8967             
##   0.5055            0.8999            0.4658               0.8967             
##   0.5043            0.8999            0.4663               0.8968             
##   0.5017            0.8977            0.4566               0.8946             
##   0.5096            0.8998            0.4649               0.8968             
##   0.5186            0.9004            0.4717               0.8973             
##   0.5184            0.9006            0.4737               0.8975             
##   0.5082            0.8994            0.4657               0.8963             
##   0.5018            0.8986            0.4607               0.8954             
##   0.5003            0.8980            0.4580               0.8951             
##   Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy
##   0.4686          0.5047       0.08491              0.7033                
##   0.4644          0.5000       0.08385              0.7003                
##   0.4722          0.5157       0.08535              0.7090                
##   0.4574          0.4932       0.08270              0.6961                
##   0.4703          0.5080       0.08492              0.7047                
##   0.4783          0.5210       0.08607              0.7122                
##   0.4616          0.5022       0.08354              0.7014                
##   0.4691          0.5073       0.08491              0.7045                
##   0.4674          0.5065       0.08450              0.7040                
##   0.4558          0.4958       0.08237              0.6972                
##   0.4755          0.5193       0.08470              0.7107                
##   0.4786          0.5234       0.08619              0.7135                
##   0.4698          0.5137       0.08490              0.7078                
##   0.4673          0.5083       0.08365              0.7044                
##   0.4819          0.5269       0.08618              0.7155                
##   0.4696          0.5066       0.08395              0.7038                
##   0.4787          0.5246       0.08607              0.7141                
##   0.4750          0.5148       0.08491              0.7086                
##   0.4776          0.5217       0.08554              0.7125                
##   0.4691          0.5154       0.08354              0.7080                
##   0.4660          0.5060       0.08289              0.7032                
##   0.4709          0.5127       0.08364              0.7068                
##   0.4684          0.5134       0.08394              0.7073                
##   0.4804          0.5287       0.08554              0.7161                
##   0.4792          0.5248       0.08501              0.7138                
##   0.4738          0.5182       0.08418              0.7100                
##   0.4624          0.5044       0.08300              0.7023                
##   0.4813          0.5281       0.08597              0.7158                
##   0.4712          0.5144       0.08375              0.7079                
##   0.4779          0.5206       0.08523              0.7117                
##   0.4601          0.5018       0.08196              0.7005                
##   0.4692          0.5121       0.08291              0.7064                
##   0.4785          0.5246       0.08491              0.7135                
##   0.4645          0.5108       0.08258              0.7052                
##   0.4678          0.5103       0.08290              0.7052                
##   0.4735          0.5202       0.08301              0.7105                
##   0.4666          0.5067       0.08258              0.7035                
##   0.4660          0.5107       0.08312              0.7055                
##   0.4685          0.5106       0.08354              0.7059                
##   0.4641          0.5054       0.08175              0.7024                
##   0.4732          0.5184       0.08353              0.7097                
##   0.4738          0.5221       0.08344              0.7114                
##   0.4708          0.5144       0.08364              0.7080                
##   0.4702          0.5167       0.08354              0.7090                
##   0.4733          0.5148       0.08322              0.7078                
##   0.4647          0.5095       0.08301              0.7050                
##   0.4704          0.5179       0.08280              0.7091                
##   0.4675          0.5115       0.08259              0.7057                
##   0.4654          0.5093       0.08206              0.7044                
##   0.4811          0.5268       0.08490              0.7149                
##   0.4779          0.5214       0.08374              0.7117                
##   0.4643          0.5066       0.08227              0.7032                
##   0.4658          0.5055       0.08228              0.7027                
##   0.4663          0.5043       0.08259              0.7021                
##   0.4566          0.5017       0.08058              0.6997                
##   0.4649          0.5096       0.08206              0.7047                
##   0.4717          0.5186       0.08302              0.7095                
##   0.4737          0.5184       0.08291              0.7095                
##   0.4657          0.5082       0.08194              0.7038                
##   0.4607          0.5018       0.08113              0.7002                
##   0.4580          0.5003       0.08069              0.6992                
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 1
## Tuning
##  parameter &#39;splitrule&#39; was held constant at a value of gini
## AUC was used to select the optimal model using the largest value.
## The final values used for the model were mtry = 1, splitrule = gini
##  and min.node.size = 72.</code></pre>
</div>

<!-- 
<div style="position:absolute;top:2px;right:10px;z-index:10000"> 
    <img src="assets/logo/cm.png" style="width:170px;margin-right:10px"/> 
    <img src="assets/logo/utfpr.png" style="width:150px"/> 
</div>
--!>



</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
